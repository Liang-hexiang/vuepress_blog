const l=JSON.parse('{"key":"v-05a88cd5","path":"/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/LLM/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B.html","title":"大语言模型简介","lang":"zh-CN","frontmatter":{"icon":"fas fa-database","date":"2025-01-03T00:00:00.000Z","cover":"https://img.tucang.cc/api/image/show/6c8acb93bd0fc9dd85006746d572df8f","category":["人工智能"],"tag":["Prompt","LLM"]},"headers":[{"level":2,"title":"1.LLM理论简介","slug":"_1-llm理论简介","link":"#_1-llm理论简介","children":[{"level":3,"title":"一、什么是LLM","slug":"一、什么是llm","link":"#一、什么是llm","children":[]}]},{"level":2,"title":"一、什么是大型语言模型（LLM）","slug":"一、什么是大型语言模型-llm","link":"#一、什么是大型语言模型-llm","children":[{"level":3,"title":"1.1 大型语言模型（LLM）的概念","slug":"_1-1-大型语言模型-llm-的概念","link":"#_1-1-大型语言模型-llm-的概念","children":[]},{"level":3,"title":"1.2 LLM 的发展历程","slug":"_1-2-llm-的发展历程","link":"#_1-2-llm-的发展历程","children":[]},{"level":3,"title":"1.3 常见的 LLM 模型","slug":"_1-3-常见的-llm-模型","link":"#_1-3-常见的-llm-模型","children":[]}]},{"level":2,"title":"二、LLM 的能力与特点","slug":"二、llm-的能力与特点","link":"#二、llm-的能力与特点","children":[{"level":3,"title":"2.1 LLM 的能力","slug":"_2-1-llm-的能力","link":"#_2-1-llm-的能力","children":[]},{"level":3,"title":"2.2 LLM 的特点","slug":"_2-2-llm-的特点","link":"#_2-2-llm-的特点","children":[]}]},{"level":2,"title":"三、LLM 的应用与影响","slug":"三、llm-的应用与影响","link":"#三、llm-的应用与影响","children":[]}],"git":{"createdTime":1739233761000,"updatedTime":1739233761000,"contributors":[{"name":"lianghexiang","email":"lhx110396@163.com","commits":1}]},"readingTime":{"minutes":22.36,"words":6708},"filePathRelative":"人工智能/LLM/大语言模型简介.md","localizedDate":"2025年1月3日","excerpt":"<h1> 大语言模型简介</h1>\\n<h2> 1.LLM理论简介</h2>\\n<h3> 一、什么是LLM</h3>\\n<p><strong>大语言模型（LLM，Large Language Model），也称大型语言模型，是一种旨在理解和生成人类语言的人工智能模型</strong>。</p>\\n<p>LLM 通常指包含<strong>数百亿（或更多）参数的语言模型</strong>，它们在海量的文本数据上进行训练，从而获得对语言深层次的理解。</p>\\n<p>为了探索性能的极限，许多研究人员开始训练越来越庞大的语言模型，例如拥有 <code>1750 亿</code>参数的 <code>GPT-3</code> 和 <code>5400 亿</code>参数的 <code>PaLM</code> 。尽管这些大型语言模型与小型语言模型（例如 <code>3.3 亿</code>参数的 <code>BERT</code> 和 <code>15 亿</code>参数的 <code>GPT-2</code>）使用相似的架构和预训练任务，但它们展现出截然不同的能力，尤其在解决复杂任务时表现出了惊人的潜力，这被称为“<strong>涌现能力</strong>”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务，而 GPT-2 在这方面表现较差。因此，科研界给这些庞大的语言模型起了个名字，称之为“大语言模型（LLM）”。LLM 的一个杰出应用就是 <strong>ChatGPT</strong> ，它是 GPT 系列 LLM 用于与人类对话式应用的大胆尝试，展现出了非常流畅和自然的表现。</p>"}');export{l as data};
